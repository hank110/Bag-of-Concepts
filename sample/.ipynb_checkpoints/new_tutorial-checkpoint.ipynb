{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial for BOC (Bag-of-Concepts)\n",
    "\n",
    "- This tutorial is a re-implementation of Kim, Han Kyul, Hyunjoong Kim, and Sungzoon Cho. \"Bag-of-Concepts: Comprehending Document Representation through Clustering Words in Distributed Representation.\" Neurocomputing (2017).\n",
    "\n",
    "- It will show you how to use this package to create sample BOC documents vectors as presented in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import the package and designate the location of the input trext file.\n",
    "\n",
    "- Sample text file contains 5,000 articles from Reuter dataset used in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bagofconcepts as bc\n",
    "\n",
    "# pip install bagofconcepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_path='./sample_data/sample_articles.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set parameters for training BOC\n",
    "\n",
    "- To train BOC, embedding dimension, context window size, minimum frequency and number of concepts must bedefined as parameters.\n",
    "- **Embedding dimension** denotes the dimensions of word vectors to be trained from word2vec\n",
    "- **Context window size** refers to the number of precedeing and subsequent words that are going to be regarded as contexts for a given input word\n",
    "- Words with frequencies below **minimum frequency** will be ignored in the model\n",
    "- ** The number of concepts** indicates the value of k to be used for spherical clustering. (number of concepts & dimensions of document vectors to be trained)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dimension=200\n",
    "context=8\n",
    "minimum_freq=50\n",
    "number_concepts=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train BOC document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-31 15:51:51,404 : INFO : collecting all words and their counts\n",
      "2018-03-31 15:51:51,415 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-03-31 15:51:51,891 : INFO : collected 69044 word types from a corpus of 2263334 raw words and 5000 sentences\n",
      "2018-03-31 15:51:51,892 : INFO : Loading a fresh vocabulary\n",
      "2018-03-31 15:51:51,919 : INFO : min_count=50 retains 4325 unique words (6% of original 69044, drops 64719)\n",
      "2018-03-31 15:51:51,920 : INFO : min_count=50 leaves 1945325 word corpus (85% of original 2263334, drops 318009)\n",
      "2018-03-31 15:51:51,929 : INFO : deleting the raw counts dictionary of 69044 items\n",
      "2018-03-31 15:51:51,932 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2018-03-31 15:51:51,933 : INFO : downsampling leaves estimated 1411610 word corpus (72.6% of prior 1945325)\n",
      "2018-03-31 15:51:51,934 : INFO : estimated required memory for 4325 words and 200 dimensions: 9082500 bytes\n",
      "2018-03-31 15:51:51,949 : INFO : resetting layer weights\n",
      "2018-03-31 15:51:52,005 : INFO : training model with 3 workers on 4325 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=8\n",
      "2018-03-31 15:51:53,031 : INFO : PROGRESS: at 3.32% examples, 236326 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:51:54,042 : INFO : PROGRESS: at 7.03% examples, 244450 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:51:55,088 : INFO : PROGRESS: at 10.53% examples, 241763 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:51:56,097 : INFO : PROGRESS: at 14.14% examples, 244097 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:51:57,125 : INFO : PROGRESS: at 17.82% examples, 245774 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:51:58,140 : INFO : PROGRESS: at 21.56% examples, 248736 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:51:59,145 : INFO : PROGRESS: at 25.26% examples, 250788 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:00,174 : INFO : PROGRESS: at 29.01% examples, 251026 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:01,222 : INFO : PROGRESS: at 32.92% examples, 252119 words/s, in_qsize 6, out_qsize 0\n",
      "2018-03-31 15:52:02,261 : INFO : PROGRESS: at 36.78% examples, 253062 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:03,311 : INFO : PROGRESS: at 40.60% examples, 253556 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:04,364 : INFO : PROGRESS: at 44.38% examples, 253975 words/s, in_qsize 6, out_qsize 0\n",
      "2018-03-31 15:52:05,404 : INFO : PROGRESS: at 48.30% examples, 254613 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:06,446 : INFO : PROGRESS: at 52.19% examples, 255164 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:07,463 : INFO : PROGRESS: at 56.05% examples, 255911 words/s, in_qsize 6, out_qsize 0\n",
      "2018-03-31 15:52:08,473 : INFO : PROGRESS: at 59.79% examples, 256350 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:09,486 : INFO : PROGRESS: at 63.31% examples, 256031 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:10,487 : INFO : PROGRESS: at 66.78% examples, 255240 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:11,511 : INFO : PROGRESS: at 70.56% examples, 255434 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:12,520 : INFO : PROGRESS: at 74.29% examples, 255616 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:13,533 : INFO : PROGRESS: at 78.12% examples, 256151 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:14,556 : INFO : PROGRESS: at 81.86% examples, 256329 words/s, in_qsize 6, out_qsize 0\n",
      "2018-03-31 15:52:15,568 : INFO : PROGRESS: at 85.58% examples, 256639 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:16,576 : INFO : PROGRESS: at 89.32% examples, 256670 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:17,620 : INFO : PROGRESS: at 93.20% examples, 256868 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:18,647 : INFO : PROGRESS: at 97.05% examples, 257103 words/s, in_qsize 5, out_qsize 0\n",
      "2018-03-31 15:52:19,384 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-03-31 15:52:19,410 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-03-31 15:52:19,423 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-03-31 15:52:19,423 : INFO : training on 11316670 raw words (7057540 effective words) took 27.4s, 257417 effective words/s\n",
      "2018-03-31 15:52:19,424 : INFO : storing 4325x200 projection weights into w2v_model_d200_w8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".... w2v_model_d200_w8 created\n",
      ".... Extracting candidate words from ./sample_data/sample_articles.txt\n",
      "    10000 th doc processed\n",
      "    20000 th doc processed\n",
      "    30000 th doc processed\n",
      "    40000 th doc processed\n",
      "    50000 th doc processed\n",
      "    60000 th doc processed\n",
      "    70000 th doc processed\n",
      "    80000 th doc processed\n",
      "    90000 th doc processed\n",
      "    100000 th doc processed\n",
      "    110000 th doc processed\n",
      "    120000 th doc processed\n",
      "    130000 th doc processed\n",
      "    140000 th doc processed\n",
      "    150000 th doc processed\n",
      "    160000 th doc processed\n",
      "    170000 th doc processed\n",
      "    180000 th doc processed\n",
      "    190000 th doc processed\n",
      "    200000 th doc processed\n",
      "    210000 th doc processed\n",
      "    220000 th doc processed\n",
      "    230000 th doc processed\n",
      "    240000 th doc processed\n",
      "    250000 th doc processed\n",
      "    260000 th doc processed\n",
      "    270000 th doc processed\n",
      "    280000 th doc processed\n",
      "    290000 th doc processed\n",
      "    300000 th doc processed\n",
      "    310000 th doc processed\n",
      "    320000 th doc processed\n",
      "    330000 th doc processed\n",
      "    340000 th doc processed\n",
      "    350000 th doc processed\n",
      "    360000 th doc processed\n",
      "    370000 th doc processed\n",
      "    380000 th doc processed\n",
      "    390000 th doc processed\n",
      "    400000 th doc processed\n",
      "    410000 th doc processed\n",
      "    420000 th doc processed\n",
      "    430000 th doc processed\n",
      "    440000 th doc processed\n",
      "    450000 th doc processed\n",
      "    460000 th doc processed\n",
      "    470000 th doc processed\n",
      "    480000 th doc processed\n",
      "    490000 th doc processed\n",
      "    500000 th doc processed\n",
      "    510000 th doc processed\n",
      "    520000 th doc processed\n",
      "    530000 th doc processed\n",
      "    540000 th doc processed\n",
      "    550000 th doc processed\n",
      "    560000 th doc processed\n",
      "    570000 th doc processed\n",
      "    580000 th doc processed\n",
      "    590000 th doc processed\n",
      "    600000 th doc processed\n",
      "    610000 th doc processed\n",
      "    620000 th doc processed\n",
      "    630000 th doc processed\n",
      "    640000 th doc processed\n",
      "    650000 th doc processed\n",
      "    660000 th doc processed\n",
      "    670000 th doc processed\n",
      "    680000 th doc processed\n",
      "    690000 th doc processed\n",
      "    700000 th doc processed\n",
      "    710000 th doc processed\n",
      "    720000 th doc processed\n",
      "    730000 th doc processed\n",
      "    740000 th doc processed\n",
      "    750000 th doc processed\n",
      "    760000 th doc processed\n",
      "    770000 th doc processed\n",
      "    780000 th doc processed\n",
      "    790000 th doc processed\n",
      "    800000 th doc processed\n",
      "    810000 th doc processed\n",
      "    820000 th doc processed\n",
      "    830000 th doc processed\n",
      "    840000 th doc processed\n",
      "    850000 th doc processed\n",
      "    860000 th doc processed\n",
      "    870000 th doc processed\n",
      "    880000 th doc processed\n",
      "    890000 th doc processed\n",
      "    900000 th doc processed\n",
      "    910000 th doc processed\n",
      "    920000 th doc processed\n",
      "    930000 th doc processed\n",
      "    940000 th doc processed\n",
      "    950000 th doc processed\n",
      "    960000 th doc processed\n",
      "    970000 th doc processed\n",
      "    980000 th doc processed\n",
      "    990000 th doc processed\n",
      "    1000000 th doc processed\n",
      "    1010000 th doc processed\n",
      "    1020000 th doc processed\n",
      "    1030000 th doc processed\n",
      "    1040000 th doc processed\n",
      "    1050000 th doc processed\n",
      "    1060000 th doc processed\n",
      "    1070000 th doc processed\n",
      "    1080000 th doc processed\n",
      "    1090000 th doc processed\n",
      "    1100000 th doc processed\n",
      "    1110000 th doc processed\n",
      "    1120000 th doc processed\n",
      "    1130000 th doc processed\n",
      "    1140000 th doc processed\n",
      "    1150000 th doc processed\n",
      "    1160000 th doc processed\n",
      "    1170000 th doc processed\n",
      "    1180000 th doc processed\n",
      "    1190000 th doc processed\n",
      "    1200000 th doc processed\n",
      "    1210000 th doc processed\n",
      "    1220000 th doc processed\n",
      "    1230000 th doc processed\n",
      "    1240000 th doc processed\n",
      "    1250000 th doc processed\n",
      "    1260000 th doc processed\n",
      "    1270000 th doc processed\n",
      "    1280000 th doc processed\n",
      "    1290000 th doc processed\n",
      "    1300000 th doc processed\n",
      "    1310000 th doc processed\n",
      "    1320000 th doc processed\n",
      "    1330000 th doc processed\n",
      "    1340000 th doc processed\n",
      "    1350000 th doc processed\n",
      "    1360000 th doc processed\n",
      "    1370000 th doc processed\n",
      "    1380000 th doc processed\n",
      "    1390000 th doc processed\n",
      "    1400000 th doc processed\n",
      "    1410000 th doc processed\n",
      "    1420000 th doc processed\n",
      "    1430000 th doc processed\n",
      "    1440000 th doc processed\n",
      "    1450000 th doc processed\n",
      "    1460000 th doc processed\n",
      "    1470000 th doc processed\n",
      "    1480000 th doc processed\n",
      "    1490000 th doc processed\n",
      "    1500000 th doc processed\n",
      "    1510000 th doc processed\n",
      "    1520000 th doc processed\n",
      "    1530000 th doc processed\n",
      "    1540000 th doc processed\n",
      "    1550000 th doc processed\n",
      "    1560000 th doc processed\n",
      "    1570000 th doc processed\n",
      "    1580000 th doc processed\n",
      "    1590000 th doc processed\n",
      "    1600000 th doc processed\n",
      "    1610000 th doc processed\n",
      "    1620000 th doc processed\n",
      "    1630000 th doc processed\n",
      "    1640000 th doc processed\n",
      "    1650000 th doc processed\n",
      "    1660000 th doc processed\n",
      "    1670000 th doc processed\n",
      "    1680000 th doc processed\n",
      "    1690000 th doc processed\n",
      "    1700000 th doc processed\n",
      "    1710000 th doc processed\n",
      "    1720000 th doc processed\n",
      "    1730000 th doc processed\n",
      "    1740000 th doc processed\n",
      "    1750000 th doc processed\n",
      "    1760000 th doc processed\n",
      "    1770000 th doc processed\n",
      "    1780000 th doc processed\n",
      "    1790000 th doc processed\n",
      "    1800000 th doc processed\n",
      "    1810000 th doc processed\n",
      "    1820000 th doc processed\n",
      "    1830000 th doc processed\n",
      "    1840000 th doc processed\n",
      "    1850000 th doc processed\n",
      "    1860000 th doc processed\n",
      "    1870000 th doc processed\n",
      "    1880000 th doc processed\n",
      "    1890000 th doc processed\n",
      "    1900000 th doc processed\n",
      "    1910000 th doc processed\n",
      "    1920000 th doc processed\n",
      "    1930000 th doc processed\n",
      "    1940000 th doc processed\n",
      "    1950000 th doc processed\n",
      "    1960000 th doc processed\n",
      "    1970000 th doc processed\n",
      "    1980000 th doc processed\n",
      "    1990000 th doc processed\n",
      "    2000000 th doc processed\n",
      "    2010000 th doc processed\n",
      "    2020000 th doc processed\n",
      "    2030000 th doc processed\n",
      "    2040000 th doc processed\n",
      "    2050000 th doc processed\n",
      "    2060000 th doc processed\n",
      "    2070000 th doc processed\n",
      "    2080000 th doc processed\n",
      "    2090000 th doc processed\n",
      "    2100000 th doc processed\n",
      "    2110000 th doc processed\n",
      "    2120000 th doc processed\n",
      "    2130000 th doc processed\n",
      "    2140000 th doc processed\n",
      "    2150000 th doc processed\n",
      "    2160000 th doc processed\n",
      "    2170000 th doc processed\n",
      "    2180000 th doc processed\n",
      "    2190000 th doc processed\n",
      "    2200000 th doc processed\n",
      "    2210000 th doc processed\n",
      "    2220000 th doc processed\n",
      "    2230000 th doc processed\n",
      "    2240000 th doc processed\n",
      "    2250000 th doc processed\n",
      "    2260000 th doc processed\n",
      ".... Min Freq<50 words removed\n",
      ".... Words have been assigned to concepts\n",
      ".... BOC vectors created in boc_d200_w8_mf50_c100.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[boc.parameters]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boc_model=boc.BOCModel(doc_path=document_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Two output files are created\n",
    "\n",
    "- ```w2c_d200_w8_mf50_c100.csv``` contains information about each word's assigned concept\n",
    "- ```boc_d200_w8_mf50_c100.csv``` contains actual BOC document vectors for the input documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Using the generated BOC document vectors as inputs, document classifiers can be trained\n",
    "\n",
    "- Using the sample articles and labels, SVM (support vector machine) will be trained to classify each document's category\n",
    "- First 4,000 articles will be used as a training data, while the rest of 1,000 articles will be used as a test data\n",
    "- 10 Fold Cross Validation is applied to search for the optimal SVM model amongst various combinations of parameters (e.g kernel type, regularization terms)\n",
    "- F1 score of prediction from test set will be printed (It will take while!)\n",
    "- Try training different types of document classifiers using BOC vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------Cross Validation Result----------\n",
      "0.75075\n",
      "{'kernel': 'linear', 'C': 0.5}\n",
      "----------Prediction Result----------\n",
      "0.758\n"
     ]
    }
   ],
   "source": [
    "from numpy import genfromtxt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "BOC_matrix=genfromtxt('boc_d200_w8_mf50_c100.csv', delimiter=',')\n",
    "\n",
    "with open('./sample_data/sample_labels.txt') as f:\n",
    "    labels=[]\n",
    "    for line in f:\n",
    "        labels.append(line)\n",
    "        \n",
    "X_train=BOC_matrix[0:4000]\n",
    "X_test=BOC_matrix[4000:]\n",
    "Y_train=labels[0:4000]\n",
    "Y_test=labels[4000:]\n",
    "\n",
    "\n",
    "parameters={'C':[0.5, 2], 'kernel':['linear', 'poly']}\n",
    "svr=svm.SVC(kernel=ek,decision_function_shape='ovr')\n",
    "clf1=GridSearchCV(svr, parameters, cv=10)\n",
    "clf1.fit(X_train, Y_train)\n",
    "print(\"----------Cross Validation Result----------\")\n",
    "print(clf1.best_score_)\n",
    "print(clf1.best_params_)\n",
    "\n",
    "\n",
    "print(\"----------Prediction Result----------\")\n",
    "yhat=clf1.predict(X_test)\n",
    "print(f1_score(Y_test, yhat, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
